{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5ad2e-7475-48e1-a34c-127e28bb3674",
   "metadata": {},
   "source": [
    "# Prompt Decomposition Example\n",
    "This notebook contains an example of prompt decomposition, or taking one long prompt and breaking it down into smaller parts.  These smaller parts can then be run independantly, often in parallel, and often on smaller models.  This can lead to significant performance enhancements and cost savings, may increase quality, and will make your prompts much easier to maintain as the workload grows.  This is because each step can be maintained and tested independantly, without any tweaks on one step impacting all the others (which may occur if they're all one huge prompt).\n",
    "\n",
    "Here are three common times when Prompt Decomposition can be helpful:\n",
    "  1) Preprocessing of RAG or context data.  For example, if context data is large, such a a long support document, consider a prompt that summarizes that content once, then future queries retrieve the summary rather than the long document. \n",
    "  2)  Breaking multi-step prompts into a maintainable DAG.  This can be helpful when a prompt reads like code, with a large number of steps or if/then instructions.  Instead, consider breaking these out and generating a flow diagram, resulting in maintainable individual pieces.\n",
    "  3)  Streamlining long linear prompts.  Even where prompts have only a single logical flow, if they are long, it can help to break the flow into small, sequential steps.  These steps combined may execute faster than the long original prompt, and they can also be maintained and tested independently. \n",
    "\n",
    "The notebook follows this structure:\n",
    "  1) Set up the envionment\n",
    "  2) Examples of decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93e1be-d464-4f6c-8019-31e15a5450a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Set up the envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de410f15-afe9-4600-845d-343fbf68b7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: anthropic in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.79.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anthropic) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.25.0->anthropic) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\disha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\disha\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735e1180-1b93-49e2-b9c5-445938069561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use Anthropics library only to count tokens locally\n",
    "import os\n",
    "from anthropic import Anthropic # type: ignore\n",
    "client = Anthropic(api_key=os.environ.get('ANTHROPIC_API_KEY', 'dummy-key'))\n",
    "def count_tokens(text):\n",
    "    return len(text.split()) * 1.3  # Rough estimate: 1.3 tokens per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e65081-5f5b-4153-97b8-333df3be2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json # type: ignore\n",
    "from botocore.config import Config # type: ignore\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6675b967-5e61-48f0-bced-2faba4fa89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v3 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "for line in models[\"modelSummaries\"]:\n",
    "    #print (line[\"modelId\"])\n",
    "    pass\n",
    "if \"anthropic.claude-3\" in str(models):\n",
    "    print(\"Claud-v3 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3a2e45-43b6-4ac4-8240-5dc3b58ca220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 1 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #for this session, do not repeat the same query to claude.\n",
    "def ask_claude(messages,system=\"\", DEBUG=False, model=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    messages can be an array of role/message pairs, or a string.\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    #print (\"Calling %s on Bedrock.  Prompt length (tokens):%s\"%(model,count_tokens(raw_prompt_text)))\n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 10000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if DEBUG: print(\"sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",\"\\n\".join(messages))\n",
    "    \n",
    "    if model== \"opus\":\n",
    "        modelId = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0'\n",
    "    elif model== \"sonnet\":\n",
    "        modelId = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0'\n",
    "    elif model== \"haiku\":\n",
    "        modelId = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "    else:\n",
    "        print (\"ERROR:  Bad model, must be opus, sonnet, or haiku.\")\n",
    "        modelId = 'us.anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "    \n",
    "    if raw_prompt_text in session_cache:\n",
    "        return [raw_prompt_text,session_cache[raw_prompt_text]]\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_prompt_text] = results\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f002e219-1bf7-4190-9b5e-0ca5acb28fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say the number four.\n",
      "Four.\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#check that it's working:\n",
    "session_cache = {} \n",
    "try:\n",
    "    query = \"Please say the number four.\"\n",
    "    #query = [{\"role\": \"user\", \"content\": \"Please say the number two.\"},{\"role\": \"assistant\", \"content\": \"Two.\"},{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "    result = ask_claude(query)\n",
    "    print(query)\n",
    "    print(result[1])\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d36a19-d377-474d-a046-4584b313c60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1],model=work[2])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,model=\"haiku\",DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i],model))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecc70b33-ec10-49e3-8d5f-0cdcde538bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\disha\\AppData\\Local\\Temp\\ipykernel_39860\\3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Please say the number one.', 'one'], ['Please say the number two.', '2'], ['Please say the number three.', 'three'], ['Please say the number four.', 'four'], ['Please say the number five.', 'five']]\n",
      "CPU times: total: 1.42 s\n",
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "session_cache = {} \n",
    "#q1 = [{\"role\": \"user\", \"content\": \"Please say the number one.\"}]\n",
    "#q2 = [{\"role\": \"user\", \"content\": \"Please say the number two.\"}]\n",
    "#q3 = [{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "#print(ask_claude_threaded([q1,q2,q3]))\n",
    "print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\",\"Please say the number three.\",\"Please say the number four.\",\"Please say the number five.\"],model='sonnet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0cb22-d6b7-44e9-b451-5dd11cea93fd",
   "metadata": {},
   "source": [
    "## 2) Examples of decomposition\n",
    "\n",
    "Here we'll consider an example use case of a user who would like to undersand how many unique characters are in a novel, and a bit about the three most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f434a-070a-4f3c-8619-acbf4fc1e478",
   "metadata": {},
   "source": [
    "### Start by downloading the novel.  Here we use Frankenstein by Mary Shelley, as it is in public domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ce4203-a672-43ec-9d45-d9f3c84c3548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3573c8e5-9d04-437e-a0b4-f5c530591913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#grab the text from the Gutenberg project, a collection of public domain works.\n",
    "#We use Beautiful Soup to parse the HTML of the webpage.\n",
    "url = \"https://www.gutenberg.org/files/84/84-h/84-h.htm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "raw_full_text_webpage = soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a425cf36-5bc9-4715-9639-2cf0aafd32cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate word count: 74984\n",
      "Approximate page count: 149\n",
      "Approximate token count: 97479.2\n"
     ]
    }
   ],
   "source": [
    "#Cut the top and bottom of the webpage so that we only have the text of the book.\n",
    "raw_full_text = raw_full_text_webpage[raw_full_text_webpage.index(\"Letter 1\\n\\nTo Mrs. Saville, England.\"):raw_full_text_webpage.find(\"*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\")].replace(\"\\r\\n\",\" \").replace(\"\\n\", \" \")\n",
    "#encode some misc unicode charaters.\n",
    "full_text = raw_full_text.encode('raw_unicode_escape').decode()\n",
    "#show that we found the expected length\n",
    "words_count = len(full_text.split())\n",
    "pages_count = int(words_count/500)#quick estimate, real page count is dependant on page and font size.\n",
    "\n",
    "token_count = count_tokens(full_text)\n",
    "print (\"Approximate word count:\",words_count)\n",
    "print (\"Approximate page count:\",pages_count)\n",
    "print (\"Approximate token count:\",token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982aa16-8f91-49f2-a5b4-14a8182d10c3",
   "metadata": {},
   "source": [
    "### Now that we have our novel, let's try to find all the unique characters with a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8518fc8-561b-4141-a990-528f18f34b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate prompt token count: 97541.6\n"
     ]
    }
   ],
   "source": [
    "long_prompt_template = \"\"\"Consider the following novel:\n",
    "<novel>\n",
    "{{NOVEL}}\n",
    "</novel>\n",
    "\n",
    "How many unique characters are there with at least one spoken line of dialog?  Please also provide a brief description of the top three most common characters in separate paragraphs. \n",
    "Only count charaters that have at least one spoken line of dialog.\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = long_prompt_template.replace(\"{{NOVEL}}\",full_text)\n",
    "print (\"Approximate prompt token count:\",count_tokens(long_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7d679c-9a1a-46b9-b8eb-9fd7bf3a3d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze the characters with spoken dialog in the novel:\n",
      "\n",
      "Characters with spoken lines:\n",
      "1. Victor Frankenstein\n",
      "2. The Monster/Creature/Daemon\n",
      "3. Elizabeth Lavenza\n",
      "4. Henry Clerval\n",
      "5. Alphonse Frankenstein (Victor's father)\n",
      "6. M. Krempe\n",
      "7. M. Waldman\n",
      "8. Justine Moritz\n",
      "9. Robert Walton\n",
      "10. The Magistrate (Mr. Kirwin)\n",
      "11. Felix De Lacey\n",
      "12. Various sailors/villagers (minor characters with brief lines)\n",
      "\n",
      "Top 3 most prominent speaking characters:\n",
      "\n",
      "Victor Frankenstein is the primary protagonist and narrator for much of the novel. He is a brilliant but obsessed scientist who creates the monster that ultimately destroys his life. His dialog reveals his passionate nature, his guilt over creating the monster, and his descent from ambitious scientist to broken man seeking revenge.\n",
      "\n",
      "The Monster/Creature is Victor's creation who becomes the antagonist, though his extensive dialogue reveals him to be articulate and complex rather than simply evil. His speeches show his evolution from an innocent being to one filled with hatred and revenge due to rejection and isolation from society. He is highly intelligent and eloquent despite his horrific appearance.\n",
      "\n",
      "Robert Walton is the frame narrator who encounters both Frankenstein and the Monster in the Arctic. Through his letters and dialogue, we see him as an ambitious explorer who serves as a parallel to Frankenstein in his dangerous pursuit of knowledge. His conversations with both Frankenstein and the Monster provide crucial perspective on the main events of the story.\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "long_responce = ask_claude(long_prompt, model=\"sonnet\")[1]\n",
    "print(long_responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe75e7-432c-4764-bef0-46ed14dd979c",
   "metadata": {},
   "source": [
    "## Not bad!  93K tokens processed.  Let's see if we can make that faster and cheaper using prompt decomposition.\n",
    "### We'll divide the novel into thirds, run each third in parallel, then write a fourth prompt to combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29a01a5-0f19-4fbe-a5db-a86607d3e329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate prompt token count: 32208.800000000003\n"
     ]
    }
   ],
   "source": [
    "short_prompt_template = \"\"\"Consider the following portion of a novel:\n",
    "<novel>\n",
    "{{NOVEL}}\n",
    "</novel>\n",
    "\n",
    "Please provide a list of unique characters, each in a character tag.  Inside the character tag should be a name tag with their name,\n",
    "a count tag with an exact count of times they appear, and a description tag with a brief description of that character.\n",
    "Only count charaters that have at least one spoken line of dialog.\n",
    "\"\"\"\n",
    "\n",
    "#let's cut the novel into thirds.\n",
    "third = int(len(full_text)/3)\n",
    "short_prompt_1 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[:third])\n",
    "short_prompt_2 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[third:third+third])\n",
    "short_prompt_3 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[third+third:])\n",
    "print (\"Approximate prompt token count:\",count_tokens(short_prompt_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8494205-5ae6-4fd6-9f87-345aa5d36b5b",
   "metadata": {},
   "source": [
    "### Now let's run these three prompts in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ca0d24-c013-4af4-a647-7ac0e426886b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\disha\\AppData\\Local\\Temp\\ipykernel_39860\\3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the characters with spoken dialog from the provided text:\n",
      "\n",
      "<character>\n",
      "<name>Victor Frankenstein</name>\n",
      "<count>3</count>\n",
      "<description>The protagonist and narrator, a young scientist who creates a monster. He is well-educated, ambitious, and becomes consumed by guilt over his creation.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>Henry Clerval</name>\n",
      "<count>4</count>\n",
      "<description>Victor's best friend who studies languages. He is kind, supportive, and helps nurse Victor back to health after his illness.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>M. Krempe</name>\n",
      "<count>3</count>\n",
      "<description>A professor of natural philosophy at Ingolstadt. He is blunt and dismissive of Victor's early studies in alchemy.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>M. Waldman</name>\n",
      "<count>2</count>\n",
      "<description>Another professor at Ingolstadt who encourages Victor's scientific pursuits. He is gentle and well-respected.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>Elizabeth Lavenza</name>\n",
      "<count>4</count>\n",
      "<description>Victor's cousin and future bride. She speaks in defense of Justine at the trial and shows great compassion.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>Justine Moritz</name>\n",
      "<count>2</count>\n",
      "<description>A servant of the Frankenstein family who is falsely accused of William's murder. She maintains her innocence at trial.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>Ernest Frankenstein</name>\n",
      "<count>3</count>\n",
      "<description>Victor's younger brother who informs him about William's death and Justine's arrest.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "<name>Alphonse Frankenstein</name>\n",
      "<count>2</count>\n",
      "<description>Victor's father who writes to inform him of William's death and tries to console the family.</description>\n",
      "</character>\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "short_responces = ask_claude_threaded([short_prompt_1,short_prompt_2,short_prompt_3],model='sonnet')\n",
    "#show the reply from one of the three prompts\n",
    "print(short_responces[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8345-7e65-42e4-a030-37e079c9869b",
   "metadata": {},
   "source": [
    "## So far it's looking good!  We've processed the whole novel in around 17 seconds, down from 42.  Let's make a final call to get a final result that matches our original long prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f49b632f-8909-4eeb-b247-9e2dcbb2669f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate prompt token count: 842.4\n"
     ]
    }
   ],
   "source": [
    "final_prompt_template = \"\"\"Consider the following list of charaters from a novel.  Each entry contains the character's name,\n",
    "a count of the number of times they appeared, and a brief description of that charater:\n",
    "<characters>\n",
    "{{CHARACTERS}}\n",
    "</characters>\n",
    "Some charaters may be listed more than once.  Use the name and description to determine that two entries are the same, \n",
    "and if they are, sum their count to support your responce.\n",
    "\n",
    "How many unique characters are there?  Please also provide a brief description of the top three most common characters in separate paragraphs. \n",
    "\"\"\"\n",
    "\n",
    "characters = short_responces[0][1]+short_responces[1][1]+short_responces[2][1]\n",
    "\n",
    "final_prompt = final_prompt_template.replace(\"{{CHARACTERS}}\",characters)\n",
    "print (\"Approximate prompt token count:\",count_tokens(final_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e003ef-8553-4c1e-b2f9-39e72a5a85bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze the unique characters and combine the counts for duplicates.\n",
      "\n",
      "There are 15 unique characters after combining duplicates:\n",
      "Victor Frankenstein (193 total appearances), The Monster/Creature/Daemon/Fiend (106 appearances), Henry Clerval (16 appearances), Elizabeth Lavenza/Elizabeth (15 appearances), Felix (7 appearances), De Lacey (6 appearances), Mr. Kirwin (6 appearances), M. Krempe (3 appearances), Ernest Frankenstein (3 appearances), M. Waldman (2 appearances), Justine Moritz (2 appearances), Alphonse Frankenstein (2 appearances), William (2 appearances), Robert Walton (4 appearances), and The Old Woman/Nurse (2 appearances).\n",
      "\n",
      "Here are descriptions of the top three most frequently appearing characters:\n",
      "\n",
      "Victor Frankenstein is the protagonist and narrator of the story, appearing 193 times throughout the novel. He is a well-educated and ambitious young scientist who creates a monster and becomes consumed by guilt over his creation. As the story progresses, he suffers greatly as his creation turns against him and kills those closest to him.\n",
      "\n",
      "The Monster/Creature/Daemon/Fiend appears 106 times in the novel. Initially gentle but rejected by society due to his hideous appearance, he becomes vengeful and intelligent. He is eloquent in his speech and desperately seeks companionship, but his rejection by humanity leads him to seek revenge against his creator by killing those close to Frankenstein.\n",
      "\n",
      "Henry Clerval, appearing 16 times, is Victor Frankenstein's closest friend from childhood. He is characterized as an optimistic and cheerful person who studies languages and loves nature and literature. He provides support to Victor and helps nurse him back to health during his illness, but ultimately becomes one of the monster's victims.\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "final_responce = ask_claude(final_prompt, model=\"sonnet\")[1]\n",
    "print(final_responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824c0df-796e-4daa-8200-27ca955773c1",
   "metadata": {},
   "source": [
    "### This final prompt took about 5 seconds to run.  The original long prompt took 42 seconds to run, and our decomposed version took 17s + 5, or 22 seconds total.  Almost twice as fast to do the same amount work!\n",
    "Note that the decomposed version actually found 11 characters, not 10.  This is somewhat common, that the quality will slightly improve with smaller, more focused prompts, because the LLM can focus more when the prompt is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c32177-3b65-480d-8e45-a94e1982dff1",
   "metadata": {},
   "source": [
    "## For fun, let's repeate the exact same tests as above, but with the smaller, faster Haiku model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ce8f17-ade2-489a-a9d5-2dc757c343e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me count the unique characters with spoken dialog:\n",
      "\n",
      "1. Victor Frankenstein\n",
      "2. The Monster/Creature\n",
      "3. Elizabeth Lavenza\n",
      "4. Robert Walton\n",
      "5. Victor's Father (Alphonse Frankenstein)\n",
      "6. Henry Clerval\n",
      "7. M. Waldman\n",
      "8. M. Krempe\n",
      "9. Justine Moritz\n",
      "10. De Lacey\n",
      "11. Felix\n",
      "12. Safie\n",
      "13. A magistrate in Ireland\n",
      "14. Mr. Kirwin\n",
      "15. A sailor/ship's leader\n",
      "16. A rustic who shoots at the monster\n",
      "\n",
      "Total: 16 unique characters with spoken dialog\n",
      "\n",
      "Top 3 Most Common Characters:\n",
      "\n",
      "Victor Frankenstein: The protagonist and creator of the monster. A brilliant but deeply troubled scientist who becomes consumed by guilt and a desire for revenge after creating life and witnessing the destruction caused by his creation.\n",
      "\n",
      "The Monster/Creature: A sentient being created by Victor Frankenstein, who is rejected by his creator and society. Initially seeking companionship and understanding, he becomes increasingly violent and vengeful after repeated experiences of rejection and isolation.\n",
      "\n",
      "Robert Walton: The narrator of the frame story, an Arctic explorer who rescues Victor Frankenstein and listens to his tale. He serves as a sympathetic listener and records Frankenstein's story in letters to his sister Margaret.\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "long_responce_haiku = ask_claude(long_prompt, model=\"haiku\")[1]\n",
    "print(long_responce_haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecc5f6-7892-4634-87dc-06f633c2e6c9",
   "metadata": {},
   "source": [
    "### Now the full prompt takes only 11s, down from about 40 with the larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36c78ec4-a818-4a97-8e5c-8e861cb836b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\disha\\AppData\\Local\\Temp\\ipykernel_39860\\3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me help you consolidate the characters and count unique entries:\n",
      "\n",
      "Unique Characters (17 total):\n",
      "1. Victor Frankenstein (total count: 134 + Numerous + Numerous = multiple)\n",
      "2. The Monster/Creature/Daemon\n",
      "3. Henry Clerval (total count: 26 + 2 + Multiple)\n",
      "4. Elizabeth Lavenza (total count: 22 + 14 + Multiple)\n",
      "5. Alphonse Frankenstein (total count: 11 + 3 + Multiple)\n",
      "6. Justine Moritz (total count: 10 + 5 + Not specified)\n",
      "7. Robert Walton\n",
      "8. Ernest Frankenstein\n",
      "9. William Frankenstein\n",
      "10. M. Waldman\n",
      "11. M. Krempe\n",
      "12. De Lacey\n",
      "13. Felix\n",
      "14. Agatha\n",
      "15. Safie\n",
      "16. Mr. Kirwin\n",
      "17. Mr. Kirwin\n",
      "\n",
      "Top Three Most Common Characters:\n",
      "\n",
      "Victor Frankenstein is the protagonist and central figure of the novel. A brilliant but tormented scientist, he creates the monster and becomes consumed by guilt and a desire for revenge, ultimately pursuing his creation across vast distances.\n",
      "\n",
      "The Monster/Creature is Frankenstein's sentient creation, rejected by society and his own creator. Seeking companionship and understanding, he becomes increasingly vengeful, murdering members of Frankenstein's family in response to his own profound loneliness and rejection.\n",
      "\n",
      "Henry Clerval is Victor's loyal and supportive best friend. Compassionate and kind, he attempts to help Victor through his psychological struggles and accompanies him on travels, ultimately becoming a victim of the monster's revenge.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "short_responces_haiku = ask_claude_threaded([short_prompt_1,short_prompt_2,short_prompt_3],model='haiku')\n",
    "characters_haiku = short_responces_haiku[0][1]+short_responces_haiku[1][1]+short_responces_haiku[2][1]\n",
    "final_prompt_haiku = final_prompt_template.replace(\"{{CHARACTERS}}\",characters_haiku)\n",
    "final_responce_haiku = ask_claude(final_prompt_haiku, model=\"haiku\")[1]\n",
    "print(final_responce_haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2a9e2-614d-4020-8039-143b6da09426",
   "metadata": {},
   "source": [
    "### Here we see that with such a fast model and small prompt, we don't get a perfomance improvment from decomposition.  Part of this is that with the shorter times involved, system operations and data transfer takes a larger percent of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4a75b-5abd-4a47-9409-cfcd999fc3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
